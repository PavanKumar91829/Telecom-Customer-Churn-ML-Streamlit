# -*- coding: utf-8 -*-
"""Telecom Customer Churn.iynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wx7asz6oJyKxID-h-VRNjbp7XdrsKtGa
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""## Load the data"""

df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Machine Learning/Project - 3 (Telecom Customer Churn)/Dataset/Telco-Customer-Churn.csv')

df.head()

## Problem Statement:
# The goal is to predict whether a telecom customer will churn based on their demographic, service usage, and billing information

## Features and Target:

# Target Variable - churn -> Indicates whether the customer has left the service (Yes or No). This is what we want to predict.

# Features - other than churn all are features

# drop column named 'customerID' which is not useful
df.drop('customerID', axis=1, inplace=True)
df.head()

data = df.copy()
data.info()

"""##EDA - exploratory data analysis"""

# Convert the column 'TotalCharges' to float
data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')

data.info()

data.shape

data.Churn.unique()

data.describe()

## Observations
#  16.2% of customers are senior citizens
#  Tenure ranges from 0 to 72 months
#  MonthlyCharges is suggesting a slightly left-skewed distribution (more customers paying less)
#  TotalCharges is indicating a right-skewed distribution

data.describe(include=['O'])

data.isna().sum()

## % of null values
data.isna().sum()/len(data)

"""## Univariate Analysis"""

def cont_vis_analysis(data,col):
  ## visualisation of continuous numerical variable
  fig,axes=plt.subplots(1,3,figsize=(8,3),layout='constrained')
  fig.suptitle('Continuous Data Distributions')

  axes[0].set_title('Histogram')
  sns.histplot(data[col],ax=axes[0])

  axes[1].set_title('KDE plot')
  sns.kdeplot(data[col],ax=axes[1])

  axes[2].set_title('Boxplot')
  sns.boxplot(data[col],ax=axes[2])

  fig.show()

def cont_non_vis_analysis(data,col):
  ## Non visual analaysis of continuous numerical data
  series=data[col]
  print(series.agg(['count','min','max','mean','median','var','std','skew','kurt']).round(2))
  print()


def catg_disc_analysis(data,col):
  print("Value Counts for ",col)
  print(data[col].value_counts())
  print("Null values =",data[col].isna().sum())
  fig,axes=plt.subplots(1,2,figsize=(8,3),layout='constrained')
  fig.suptitle('Categorical / Discrete Num Data Distributions')

  axes[0].set_title('Bar Plot')
  sns.countplot(x=col,data=data,ax=axes[0])

  axes[1].set_title('Pie Chart')
  vc_data=data[col].value_counts()
  axes[1].pie(vc_data,labels=vc_data.index,autopct='%1.1f%%')
  fig.show()

print("Non Visual Analysis")
cont_non_vis_analysis(data,'tenure')
print("Visual Analysis")
cont_vis_analysis(data,'tenure')

## Observations in "tenure"
# 1. There are no outliers in tenure data
# 2. KDE plot of tenure is a platykurtic distribution
# 3. Tenure distribution is slightly right-skewed

print("Non Visual Analysis")
cont_non_vis_analysis(data,'MonthlyCharges')
print("Visual Analysis")
cont_vis_analysis(data,'MonthlyCharges')

## Observations in "MonthlyCharges"
# 1. There are no outliers in MonthlyCharges data
# 2. KDE plot of MonthlyCharges is a platykurtic distribution
# 3. MonthlyCharges distribution is left-skewed

print("Non Visual Analysis")
cont_non_vis_analysis(data,'TotalCharges')
print("Visual Analysis")
cont_vis_analysis(data,'TotalCharges')

## Observations in "TotalCharges"
# 1. There are no outliers in TotalCharges data
# 2. KDE plot of TotalCharges is a platykurtic distribution
# 3. TotalCharges distribution is right-skewed

data.columns

catg_disc_analysis(data,'gender')

catg_disc_analysis(data,'SeniorCitizen')

catg_disc_analysis(data,'PhoneService')

catg_disc_analysis(data,'MultipleLines')

catg_disc_analysis(data,'InternetService')

catg_disc_analysis(data,'OnlineSecurity')

catg_disc_analysis(data,'OnlineBackup')

catg_disc_analysis(data,'DeviceProtection')

catg_disc_analysis(data,'TechSupport')

catg_disc_analysis(data,'StreamingTV')

catg_disc_analysis(data,'StreamingMovies')

catg_disc_analysis(data,'Contract')

catg_disc_analysis(data,'PaperlessBilling')

catg_disc_analysis(data,'PaymentMethod')

catg_disc_analysis(data,'Churn')

"""## Bivariate Analysis"""

sns.pairplot(data, hue='Churn')

# cont - cat bivariate
def cont_cat_bivar_analysis(data,cat_col,num_col):
  plt.figure(figsize=(3,3))
  sns.boxplot(x=cat_col,y=num_col,data=data)

cont_cat_bivar_analysis(data,'Churn','tenure')

## Observations
#  There are some outliers present while plotting boxplot tenure vs Churn
#  Most churners leave within their first year

cont_cat_bivar_analysis(data,'Churn','MonthlyCharges')

cont_cat_bivar_analysis(data,'Churn','TotalCharges')

## Observations
#  There are outliers present in TotalCharges plot vs Churn
#  There median of TotalCharges whose Churn is Yes less than Churn No
#  This info might be wrong due to more outliers present in Churn Yes

## cat - cat bivariate
def cat_cat_bivar_analysis(data,col1,col2):
  ct_data=pd.crosstab(data[col1],data[col2])
  ct_data.plot(kind='bar')

categorical_cols = [
    'Contract', 'InternetService', 'OnlineSecurity',
    'PaymentMethod', 'TechSupport','PaperlessBilling', 'DeviceProtection'
]

for col in categorical_cols:
    cat_cat_bivar_analysis(data, col, 'Churn')

"""## Data Preprocessing"""

# There are 11 null values in Total Charges
# So drop the rows which has less affect on total data
data.dropna(inplace=True)

data.isna().sum()

data.head()

## duplicates check
data.duplicated().sum()

## remove duplicate rows from data
data.drop_duplicates(inplace=True)
data.duplicated().sum()

data.shape

"""## Label Encoding"""

data.head()

from sklearn.preprocessing import LabelEncoder

# Make a copy of original data
data_encoded = data.copy()

# Columns that require Label Encoding
cols_to_encode = [
    'gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',
    'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',
    'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract',
    'PaperlessBilling', 'PaymentMethod', 'Churn'
]

# Apply Label Encoding
label_encoders = {}
for col in cols_to_encode:
    le = LabelEncoder()
    data_encoded[col] = le.fit_transform(data_encoded[col])
    label_encoders[col] = le


data_encoded.head()

"""## Train_Test_Split"""

# Using features predict Churn is Yes or No
x=data_encoded.drop('Churn',axis=1) ## features
y=data_encoded['Churn'] ## target
x.shape,y.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape, y_train.shape, y_test.shape

X_train.head()

"""## Standard Scaling

"""

from sklearn.preprocessing import MinMaxScaler

# Create a MinMaxScaler object
scaler = MinMaxScaler()

# Fit the scaler to the training data and transform it
X_train[['tenure', 'MonthlyCharges', 'TotalCharges']] = scaler.fit_transform(X_train[['tenure', 'MonthlyCharges', 'TotalCharges']])

# Transform the testing data using the fitted scaler
X_test[['tenure', 'MonthlyCharges', 'TotalCharges']] = scaler.transform(X_test[['tenure', 'MonthlyCharges', 'TotalCharges']])

X_train.head()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Initialize the classifier
classifier = LogisticRegression(random_state=42)

# 1. Train the model on scaled training data
classifier.fit(X_train, y_train)

# 2. Make predictions on the scaled test data
y_pred = classifier.predict(X_test)

# 3. Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Print the results
print(f"Accuracy: {accuracy}")
print(f"Confusion Matrix:\n{conf_matrix}")
print(f"Classification Report:\n{class_report}")

